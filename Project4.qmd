---
title: "Project Part 4"
format: html
---


	Late in 2018, Amazon decided to abandon a tool that they had been working on which used a machine learning algorithm to make hiring decisions by observing patterns from past resumes. According to an article from the BBC, the artificial intelligence system was launched in 2014 and was trained on resume data from the past ten years. It would review the resumes of candidates and give them a rating of one to five stars based on their hireability, similar to the way that consumers are able to review Amazon products. 
	The program was ultimately scrapped due to various issues with the fairness and the effectiveness of the tool. The first of these was the tool learning to favor male candidates throughout the process, making them more likely to gain higher ratings and subsequently be hired. According to Reuters, the algorithm taught itself to punish resumes with the word “women” in them, with a prominent example being one resume who’s owner was “women’s chess club captain”. While Amazon quickly edited the tool to ensure this was taken out of the system, that didn’t sufficiently address other concerns, many of them stemming from the fact that the training data included mostly male resumes, due to the dominance of men in the tech industry over the past decade. The other concern with the tool aside from discrimination, according to the BBC, is that it simply wasn’t good at identifying candidates that were qualified for the position, routinely giving high scores to candidates without the necessary education and experience. Now let us get into some of the underlying ethical concerns behind this system. 
	A very obvious way to start is by asking the question: are those individuals (measured) representative of the people to whom we’d like to apply the algorithm? As I touched on briefly in the above paragraph, the answer is no. The training data consisted mostly of resumes from past male candidates, causing a clear disadvantage to the female candidates applying for jobs. Some of this discrimination included knocking the use of the word ‘women’, but according to Reuters the technology also favored those who used more masculine language in their resumes. This would lead to the system favoring those resumes that were more characteristically male, and thus creating unfair advantages in the hiring process. 
	Another question that we must look at is this: should gender be a variable? The Civil Rights Act of 1964 states that companies may not discriminate based on gender or race when hiring, and so obviously Amazon wouldn’t have included those as variables in their hiring algorithm. That doesn’t mean however, that the system wasn’t able to pick up which resumes had characteristics more likely to belong to a man and which had characteristics more likely to belong to a woman and use that in the decision making process. While Amazon denies that this algorithm was ever used for hiring decisions, it is clear based on evidence stated above that using it for those purposes would undoubtedly be illegal. 
	For our last two ethical questions about this application of data science, we will use the Data Science Principles and Manifesto as our reference. I want to specifically examine this entry: Build teams with diverse ideas, backgrounds, and strengths. Does this tool help Amazon accomplish that? Well we know that the algorithm favored men over women, and it is likely that the past workers at Amazon were primarily white, so if the system was trained using their resumes, it is likely that there were other discriminatory inclinations of this machine beyond just women. 
	Our final question that we will examine is: Recognize and mitigate bias in ourselves and in the data we use. To Amazon’s credit, they found that the methods that they were using contained bias, took steps to mitigate the bias, and eventually when it became clear that that would be too difficult, eliminated the algorithm altogether. According to Reuters, Amazon claimed that the information from the algorithm was never used in hiring decisions although recruiters had looked at it. This is a lesson that it is important to consider the biases in our data or systems before we use it for decision making purposes. 
	There is a lot we can learn from the situation that arose from Amazon’s attempt at making an algorithm to read resumes and rate candidates based on those resumes. Among those lessons: Recognize bias, build diverse teams, investigate your variables and make sure your training data fits with what you are trying to use your algorithm for. 

BBC News. “Amazon Scrapped 'Sexist AI' Tool.” BBC News, 10 Oct. 2018, https://www.bbc.com/news/technology-45809919.

Dastin, Jeffrey. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women.” Reuters, 10 Oct. 2018, https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/.

